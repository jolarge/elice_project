{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e0b1803-58ae-4287-8a8f-161aca3e2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "#import torch.nn.functional as F\n",
    "#import torch.optim as optim\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "#import numpy as np\n",
    "#from tqdm import tqdm, tqdm_notebook\n",
    "\"\"\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2ef1353-9bd9-494d-a3c7-bb2620d6a282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "673236c7-8554-4843-96b2-8c44f47e3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kobert.utils import get_tokenizer\n",
    "# from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "384ce7d1-331e-4b28-9497-1aead4203b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bertmodel, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bf7e59-9722-4f4a-9d7c-e29a062ce62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = BERTDataset(dataset_train[:100], 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837d1df5-9175-4063-8024-80cf650fd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac4553c7-3b4b-46d5-b464-27a9a95d05bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = get_tokenizer()\n",
    "# tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c84ed57b-0979-4998-95d9-2c352c5f5bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##GPU 사용 시\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58edb51b-9079-46dc-9892-27303e683e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df82ab3b-7a7a-4051-94c9-1136a80b239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88be2ed-fa6b-4205-a4be-32704dcb3655",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_model = DistilBertModel.from_pretrained('monologg/distilkobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ac97d1-b9c2-471e-8951-b289ca3e5d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertModel(\n",
       "  (embeddings): Embeddings(\n",
       "    (word_embeddings): Embedding(8002, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (layer): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (attention): MultiHeadSelfAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (ffn): FFN(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilbert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "735040cb-70eb-4ef0-90c7-a4d36f03d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_kobert import KoBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec5c3aee-5fa5-448f-897b-a88d177a829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoBertTokenizer.from_pretrained('monologg/distilkobert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6661d2e-98d6-4fc4-bbba-f289f79a5eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='monologg/distilkobert', vocab_size=8002, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a897cd3c-7a4d-4a33-a961-c27e722e3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a4685d-64ee-4e2d-bb07-9e8a82365ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv('../labeling/data/data3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c810d85d-c612-4fb9-a1b9-dfe8615ec567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>사설 北 핵·ICBM 시험 재개 시사… ‘잘못된 길’ 선택해선 안 된다</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>사설 수출 역성장 멈추려면 기업 활력부터 되살려내야</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>사설 여권 일방통행식 국회 운영 정국 정상화 가능하겠나</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>설왕설래 인구절벽</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>세계포럼 친중 정책 레드라인 넘지 말아야</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091103</th>\n",
       "      <td>고무줄 면역효과 불구 영국은 왜 아스트라제네카를 승인했나</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091104</th>\n",
       "      <td>박원순의 마지막 이틀 부인→고백→대응→포기 흔들렸다</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091105</th>\n",
       "      <td>경기도 공공배달앱 ‘배달특급’ 20여일만에 거래액 20억 돌파</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091106</th>\n",
       "      <td>알림뉴시스 콘텐츠 저작권 고지</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091107</th>\n",
       "      <td>녹유 오늘의 운세51년생 침이 고여지는 대접을 받아요</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6091108 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text_headline  label\n",
       "0        사설 北 핵·ICBM 시험 재개 시사… ‘잘못된 길’ 선택해선 안 된다      1\n",
       "1                   사설 수출 역성장 멈추려면 기업 활력부터 되살려내야      2\n",
       "2                 사설 여권 일방통행식 국회 운영 정국 정상화 가능하겠나      2\n",
       "3                                      설왕설래 인구절벽      1\n",
       "4                         세계포럼 친중 정책 레드라인 넘지 말아야      2\n",
       "...                                          ...    ...\n",
       "6091103          고무줄 면역효과 불구 영국은 왜 아스트라제네카를 승인했나      2\n",
       "6091104             박원순의 마지막 이틀 부인→고백→대응→포기 흔들렸다      2\n",
       "6091105       경기도 공공배달앱 ‘배달특급’ 20여일만에 거래액 20억 돌파      2\n",
       "6091106                         알림뉴시스 콘텐츠 저작권 고지      0\n",
       "6091107            녹유 오늘의 운세51년생 침이 고여지는 대접을 받아요      0\n",
       "\n",
       "[6091108 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3edaccca-6a12-44cb-98f9-3c95b1bdd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bea9b27-092d-4e4f-8c0b-91fbfbcf3c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6091108it [00:15, 389338.61it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = []\n",
    "for q, label in tqdm(zip(df_data['text_headline'], df_data['label'])):\n",
    "    data = []\n",
    "    clean_title = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…\\\"\\“》]', ' ', str(q)) \n",
    "    data.append(clean_title)\n",
    "    data.append(str(label))\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4d4b07e-ef5d-439b-a7dc-7563c395cb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['사설 北 핵·ICBM 시험 재개 시사   잘못된 길’ 선택해선 안 된다', '1']\n",
      "['돋을새김 전광훈과 한국당', '1']\n",
      "['이총리 총선 탄핵 전후 분출된 국민요구 해결·지체의 분수령', '1']\n",
      "['2030 리스펙트 미디어 정치와 정당의 위기  허승규', '1']\n",
      "['새해 첫날 전국 가끔 구름 많고 일부 중부지방에 눈발', '1']\n",
      "['녹유 오늘의 운세51년생 침이 고여지는 대접을 받아요', '0']\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0])\n",
    "print(data_list[6000])\n",
    "print(data_list[12000])\n",
    "print(data_list[18000])\n",
    "print(data_list[19999])\n",
    "print(data_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "069f6f12-e1d1-4c31-8084-38eb6492162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 셋 test와 train으로 4:1분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "dataset_train, dataset_test = train_test_split(data_list, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f52533b-17b2-4a45-8b99-4c31d27752be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4cf46333-d867-4b80-b0c6-4fc254b13b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERTDataset(Dataset):\n",
    "#     def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "#                  pad, pair):\n",
    "#         start = time.time()\n",
    "#         transform = nlp.data.BERTSentenceTransform(\n",
    "#             bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "#         self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "#         self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "#         end = time.time()\n",
    "#         print(\"Using time : \", round(end-start,2), end='\\n')\n",
    "#     def __getitem__(self, i):\n",
    "#         return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return (len(self.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c61badf0-ecaf-4b99-8f5a-318cb0cacfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        tokens, masks, segments, targets = [], [], [], []\n",
    "        \n",
    "        start = time.time()\n",
    "        # 문장을 토큰화\n",
    "    \n",
    "        # tokens = [bert_tokenizer.encode(d[sent_idx], max_length = max_len, pad_to_max_length=pad) for d in dataset]\n",
    "        sentenses = [sentense[0] for sentense in dataset]\n",
    "\n",
    "        tokens = bert_tokenizer.batch_encode_plus(sentenses, max_length = max_len, padding = pad)\n",
    "       \n",
    "        # 마스크 토큰화 한 부분 - 1 아닌 부분 0\n",
    "        # num_zeros = [t.count(1) for t in tokens]\n",
    "        \n",
    "\n",
    "        # masks = [[1] * (max_len-num_zero) + [0] * num_zero for num_zero in num_zeros]\n",
    "\n",
    "            # 문장의 전후 관계를 구분해주는 세그먼트는 문장이 1개 밖에 없으므로 모두 0\n",
    "\n",
    "        # segments = [np.int32(max_len) for i in range(len(tokens))]\n",
    "\n",
    "        targets = [np.int32(d[label_idx]) for d in dataset] \n",
    "        \n",
    "        masks = tokens['attention_mask']\n",
    "        \n",
    "        valid_length = [mask.count(1) for mask in masks]\n",
    "        \n",
    "        # segments = tokens['token_type_ids']\n",
    "        tokens = tokens['input_ids']\n",
    "         \n",
    "        \n",
    "        self.valid_length = np.array(valid_length)\n",
    "        self.tokens = np.array(tokens)\n",
    "        self.masks = np.array(masks)\n",
    "        # self.segments = np.array(segments)\n",
    "        self.targets = np.array(targets)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Using time : \", round(end-start,2), end ='\\n')\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return (self.tokens[i], self.valid_length[i], self.masks[i], self.targets[i])\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ddb83-2f92-4a0f-8bb2-e38a5b482edb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ffdfa10-64b3-4b7f-b707-8625ad9a2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 64\n",
    "batch_size = 64\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 2\n",
    "max_grad_norm = 1\n",
    "log_interval = 1000\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34acb789-d87f-43d3-b5c2-a7fbb7eccf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using time :  0.01\n"
     ]
    }
   ],
   "source": [
    "data_train = BERTDataset(dataset_train[:100], 0, 1, tokenizer, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09abcb6c-5fa6-47ea-beb0-305c6417dcfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 3332, 7234, 5495, 4446, 5330, 5760, 4487,  517, 7169, 7823,\n",
       "        3517,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1]),\n",
       " 13,\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0]),\n",
       " 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59e9107f-1f21-44a0-9079-d256da6184d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using time :  489.48\n",
      "Using time :  157.47\n"
     ]
    }
   ],
   "source": [
    "data_train = BERTDataset(dataset_train, 0, 1, tokenizer, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tokenizer, max_len, True, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33bd9c9a-2f98-4d16-9625-6fc175875ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   2, 3332, 7234, 5495, 4446, 5330, 5760, 4487,  517, 7169, 7823,\n",
       "        3517,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1]),\n",
       " 13,\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]),\n",
       " 2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a17848c-cc51-4371-84cb-dcdbe1301eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=0)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87b4193a-8f5b-458b-87d5-d74c9c50002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes=3,   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        # _, pooler = self.bert(input_ids = token_ids, attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        output = self.bert(input_ids = token_ids, attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        # print(output[0][:,0])\n",
    "        # print(output)\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(output[0][:,0])\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b67dc6ea-03dc-46f2-ab3f-dfbe14593a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(distilbert_model,  dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b27f458-1e82-49c1-8027-5c6c861ad00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b03e550-f214-4d75-bf1c-72fcd3b5aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bdee50f-dcdd-4ec6-b205-b63c16a1798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2ff8796-f075-4f79-be79-00b1e69fcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13f2c0d9-6f1d-464d-8e09-05d36686077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    # print(max_vals, max_indices,Y)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734c4ce9-1899-47de-9d69-401ae9e160b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeon\\anaconda3\\envs\\stockpredict\\lib\\site-packages\\ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6185f086be444a0b0222ad4285a733a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 0.1475244015455246 train acc 0.953125\n",
      "epoch 1 batch id 1001 loss 0.1340157836675644 train acc 0.9549825174825175\n",
      "epoch 1 batch id 2001 loss 0.011229909025132656 train acc 0.9552411294352824\n",
      "epoch 1 batch id 3001 loss 0.04438058286905289 train acc 0.9555252415861379\n",
      "epoch 1 batch id 4001 loss 0.09414537250995636 train acc 0.9555657960509872\n",
      "epoch 1 batch id 5001 loss 0.07994286715984344 train acc 0.9559775544891022\n",
      "epoch 1 batch id 6001 loss 0.11352035403251648 train acc 0.9561375187468755\n",
      "epoch 1 batch id 7001 loss 0.054013825953006744 train acc 0.9560955577774604\n",
      "epoch 1 batch id 8001 loss 0.19321367144584656 train acc 0.9562359392575928\n"
     ]
    }
   ],
   "source": [
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    predict_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        # segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        # print(token_ids, valid_length)\n",
    "        out = model(token_ids, valid_length)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        # segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "    torch.save(model, '../model/distilmodel_1_{}.pt'.format(e+2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae34fd97-a8e9-47fe-8487-6f657555b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../model/distilmodel_1_{}.pt'.format(e+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26388b21-f0c5-4ac6-90aa-284ca0dd80aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
